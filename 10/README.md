# 第10章 回帰分析

## 10.1 回帰分析の概要

回帰分析は、目的変数Yと説明変数Xの間に成り立つ関係式を求める分析手法である。回帰分析には、説明変数が1つの**単純回帰分析**と、説明変数が複数ある**重回帰分析**がある。

### 単純回帰

単純回帰分析では、直線的な関係を仮定し、以下の式で表される。

$$
Y = a + bX
$$

ここで、

*   Y: 目的変数（従属変数）
*   X: 説明変数（独立変数）
*   a: 切片（X=0のときのYの値）
*   b: 傾き（Xが1単位変化したときのYの変化量）

この式は、Xの値が与えられたときに、Yの値を予測するためのモデルとなる。回帰分析の目的は、データに基づいて最適な *a* と *b* の値を求めることである。教科書にも記述があるように、回帰式は、観測点の間を通って、できるだけ合理的に引いた線である。

### 重回帰

説明変数が複数ある場合、以下の式で表される。

$$
Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_kX_k
$$

ここで、

*   Y: 目的変数（従属変数）
*   X<sub>1</sub>, X<sub>2</sub>, ..., X<sub>k</sub>: 説明変数（独立変数）
*   β<sub>0</sub>: 切片
*   β<sub>1</sub>, β<sub>2</sub>, ..., β<sub>k</sub>: 各説明変数に対応する偏回帰係数

重回帰分析では、各説明変数がYに与える影響を個別に評価することができる。

## 10.2 OLS（最小二乗法）

最適な *a* と *b* （またはβ<sub>0</sub>, β<sub>1</sub>, ..., β<sub>k</sub>）の値を求める方法として、最も一般的に用いられるのが最小二乗法（OLS：Ordinary Least Squares）である。教科書にも記述があるように、残差（実際のデータと回帰直線との誤差）の二乗和を最小にするように係数を決定する方法である。残差そのものはプラスとマイナスがあるので、二乗和を用いることで、残差の大きさを適切に評価する。

### 単純回帰におけるOLS

単純回帰におけるOLSでは、以下の手順で *a* と *b* を求める。

1.  残差の二乗和を定義する。

$$
\sum_{i=1}^{n}(Y_i - (a + bX_i))^2
$$

2.  この二乗和を *a* と *b* で偏微分し、それぞれ0とおくことで、以下の連立方程式を得る。

$$
\begin{cases}
\sum_{i=1}^{n} Y_i = na + b\sum_{i=1}^{n} X_i \\
\sum_{i=1}^{n} X_iY_i = a\sum_{i=1}^{n} X_i + b\sum_{i=1}^{n} X_i^2
\end{cases}
$$

3.  この連立方程式を解くことで、*a* と *b* の値を求める。解は以下のようになる。

$$
b = \frac{\sum_{i=1}^{n}(X_i - \bar{X})(Y_i - \bar{Y})}{\sum_{i=1}^{n}(X_i - \bar{X})^2} = \frac{\sum_{i=1}^{n}X_iY_i - n\bar{X}\bar{Y}}{\sum_{i=1}^{n}X_i^2 - n\bar{X}^2}
$$

$$
a = \bar{Y} - b\bar{X}
$$

これらの式を用いることで、データに基づいて最適な回帰直線を求めることができる。教科書にある注意書きとして、OLSを用いて回帰式を推定する際には、推定する式が理論的に意味のある式でなければならないという点がある。無意味な変数同士で回帰分析を行っても、意味のある結果は得られない。

## 10.3 Classified data の OLS

分類データ（Classified data）の場合、各階級の中央値を用いてOLSを適用する。基本的な考え方は生データの場合と同じだが、各階級の度数を考慮する必要がある。

具体的には、上記の式において、各データ点(X<sub>i</sub>, Y<sub>i</sub>)の代わりに、各階級の中央値(x<sub>i</sub>, y<sub>i</sub>)を用い、各項にその階級の度数f<sub>i</sub>を乗じる。例えば、 $\sum XY$ は $\sum x_iy_if_i$ に置き換えられる。

## 10.4 もっと詳しい回帰分析 - 回帰係数の有意性検定

求められた回帰係数 *b* （またはβ<sub>i</sub>）が統計的に有意であるかどうかを検定することができる。これは、説明変数Xが目的変数Yに本当に影響を与えているのかどうかを判断するために重要である。

回帰係数の有意性検定は、以下の手順で行う。

1.  **帰無仮説（H<sub>0</sub>）：** 回帰係数 *b* （またはβ<sub>i</sub>）は0である（XはYに影響を与えていない）。
2.  **対立仮説（H<sub>1</sub>）：** 回帰係数 *b* （またはβ<sub>i</sub>）は0ではない（XはYに影響を与えている）。
3.  **検定統計量tの計算：**

$$
t = \frac{b - 0}{S_b}
$$

ここで、S<sub>b</sub>は回帰係数 *b* の標準誤差である。

4.  **t分布を用いてp値を計算するか、棄却域を設定する。**

5.  p値が有意水準α以下であれば、帰無仮説を棄却し、回帰係数 *b* は統計的に有意であると結論付ける。

回帰分析を学ぶことで、変数間の関係を数式で表現し、予測モデルを構築できるようになった。特に、最小二乗法を用いて最適な回帰直線を求める方法、そして回帰係数の有意性検定によって、その関係が統計的に意味があるのかを判断できるようになったことは大きな学びである。